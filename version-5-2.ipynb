{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:34:48.050605Z","iopub.execute_input":"2025-04-19T01:34:48.051207Z","iopub.status.idle":"2025-04-19T01:34:48.059710Z","shell.execute_reply.started":"2025-04-19T01:34:48.051183Z","shell.execute_reply":"2025-04-19T01:34:48.058875Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset, Dataset\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport pandas as pd\nimport pickle\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:34:48.060959Z","iopub.execute_input":"2025-04-19T01:34:48.061465Z","iopub.status.idle":"2025-04-19T01:34:48.093888Z","shell.execute_reply.started":"2025-04-19T01:34:48.061449Z","shell.execute_reply":"2025-04-19T01:34:48.093215Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"dataset = load_dataset(\"ag_news\")\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\n\ndef tokenize_function(examples):\n    texts = [text.replace('\\n', ' ').strip() for text in examples[\"text\"]]\n    texts = [text.replace('&quot;', '\"').replace('&apos;', \"'\").replace('&amp;', '&') for text in texts]\n    \n    return tokenizer(\n        texts, \n        truncation=True, \n        padding=\"max_length\", \n        max_length=128,  # Increased from 128 to 256\n        return_tensors=\"pt\"\n    )\n\n# Apply tokenization to the dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\ntokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:34:48.094637Z","iopub.execute_input":"2025-04-19T01:34:48.094826Z","iopub.status.idle":"2025-04-19T01:35:12.363623Z","shell.execute_reply.started":"2025-04-19T01:34:48.094812Z","shell.execute_reply":"2025-04-19T01:35:12.362614Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25c5eae366c7459a89747f1a7ab6f306"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"400c8e12353a4da79ca1a4a53b6faed5"}},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=4)\n\n# Configure the LoRA adapter\nlora_config = LoraConfig(\n    r=4,  # Increased rank for better adaptation\n    lora_alpha=16,  # Reduced alpha for smoother adjustments\n    target_modules=[\"query\",\"value\",\"output.dense\"],  # Applying LoRA to specific layers\n    lora_dropout=0.05,  # Keeps dropout to regularize the adaptation\n    bias=\"none\",  \n    task_type=TaskType.SEQ_CLS  # Sequence classification task\n)\n\n# Attach the LoRA adapter to the model\nmodel = get_peft_model(model, lora_config)\nmodel.to(device)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:35:12.366085Z","iopub.execute_input":"2025-04-19T01:35:12.366368Z","iopub.status.idle":"2025-04-19T01:35:12.772456Z","shell.execute_reply.started":"2025-04-19T01:35:12.366342Z","shell.execute_reply":"2025-04-19T01:35:12.771785Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 999,172 || all params: 125,647,880 || trainable%: 0.7952\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",  # Directory to store results\n    eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n    save_strategy=\"epoch\",  # Save model after every epoch\n    learning_rate=3e-5,  # Adjusted learning rate for faster convergence\n    per_device_train_batch_size=8,  # Adjusted batch size for stability\n    per_device_eval_batch_size=64,  # Evaluation batch size\n    num_train_epochs=3,  # Increased epochs to train more thoroughly\n    weight_decay=0.01,  # Regularization to prevent overfitting\n    logging_dir=\"./logs\",  # Log directory for tracking\n    load_best_model_at_end=True,  # Load the best model at the end\n    metric_for_best_model=\"accuracy\",  # Use accuracy for model selection\n    report_to=\"none\",  # Disable logging to external services\n    lr_scheduler_type=\"cosine_with_restarts\",\n    label_smoothing_factor=0.05,\n    warmup_ratio=0.15,\n    eval_steps=500,\n    save_steps=500,\n    logging_steps=100,\n    optim=\"adamw_torch\",  \n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-8,\n    greater_is_better=True,\n    bf16=True,\n    gradient_accumulation_steps=4  # Add gradient accumulation for stability\n)\n\n# Function to compute accuracy during evaluation\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"accuracy\": accuracy_score(labels, predictions)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:35:12.774603Z","iopub.execute_input":"2025-04-19T01:35:12.774822Z","iopub.status.idle":"2025-04-19T01:35:12.807060Z","shell.execute_reply.started":"2025-04-19T01:35:12.774805Z","shell.execute_reply":"2025-04-19T01:35:12.806562Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:35:12.807965Z","iopub.execute_input":"2025-04-19T01:35:12.808237Z","iopub.status.idle":"2025-04-19T01:35:12.854772Z","shell.execute_reply.started":"2025-04-19T01:35:12.808216Z","shell.execute_reply":"2025-04-19T01:35:12.853957Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1236968066.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:35:12.855404Z","iopub.execute_input":"2025-04-19T01:35:12.855559Z","iopub.status.idle":"2025-04-19T03:06:40.264631Z","shell.execute_reply.started":"2025-04-19T01:35:12.855546Z","shell.execute_reply":"2025-04-19T03:06:40.263902Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5625/5625 1:31:25, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.404200</td>\n      <td>0.399560</td>\n      <td>0.917500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.388100</td>\n      <td>0.374885</td>\n      <td>0.926711</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.366200</td>\n      <td>0.371921</td>\n      <td>0.928553</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5625, training_loss=0.4537044475979275, metrics={'train_runtime': 5486.8882, 'train_samples_per_second': 65.611, 'train_steps_per_second': 1.025, 'total_flos': 2.39566712832e+16, 'train_loss': 0.4537044475979275, 'epoch': 3.0})"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"output_dir = \"saved_model\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save the model\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"✅ Model and tokenizer saved to {output_dir}\")\n\n# Save training arguments\nimport json\ntraining_args_dict = training_args.to_dict()\nwith open(os.path.join(output_dir, \"training_args.json\"), \"w\") as f:\n    json.dump(training_args_dict, f, indent=2)\nprint(\"✅ Training arguments saved\")\n\n# Save model config\nmodel.config.save_pretrained(output_dir)\nprint(\"✅ Model config saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T03:06:40.265441Z","iopub.execute_input":"2025-04-19T03:06:40.265670Z","iopub.status.idle":"2025-04-19T03:06:40.430815Z","shell.execute_reply.started":"2025-04-19T03:06:40.265652Z","shell.execute_reply":"2025-04-19T03:06:40.430233Z"}},"outputs":[{"name":"stdout","text":"✅ Model and tokenizer saved to saved_model\n✅ Training arguments saved\n✅ Model config saved\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\n# Load the saved config\nconfig = PeftConfig.from_pretrained(\"saved_model\")\n\n# Load the base model\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    \"roberta-base\",\n    num_labels=4,\n    device_map=\"auto\"\n)\n\n# Load the fine-tuned model with adapters\nloaded_model = PeftModel.from_pretrained(\n    base_model,\n    \"saved_model\",\n    device_map=\"auto\"\n)\n\n# Load the tokenizer\nloaded_tokenizer = AutoTokenizer.from_pretrained(\"saved_model\")\n\nprint(\"✅ Model and tokenizer loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T03:06:40.431439Z","iopub.execute_input":"2025-04-19T03:06:40.431634Z","iopub.status.idle":"2025-04-19T03:06:41.385826Z","shell.execute_reply.started":"2025-04-19T03:06:40.431619Z","shell.execute_reply":"2025-04-19T03:06:41.385047Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"✅ Model and tokenizer loaded successfully\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nwith open(\"/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\", \"rb\") as f:\n    test_dataset = pickle.load(f)\nprint(f\"✅ Loaded raw test dataset with {len(test_dataset['text'])} examples\")\n\n# Convert to HuggingFace Dataset format\ntest_dataset = Dataset.from_dict({\"text\": test_dataset[\"text\"]})\nprint(\"✅ Converted to HuggingFace Dataset format\")\n\n# Tokenize test data\ndef preprocess_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128\n    )\n\ntokenized_test = test_dataset.map(preprocess_function, batched=True)\ntokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\nprint(\"✅ Tokenization complete\")\n\n# Create DataLoader for batched predictions\ntest_dataloader = DataLoader(tokenized_test, batch_size=64)\nprint(f\"✅ Created DataLoader with {len(test_dataloader)} batches\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T03:06:41.387953Z","iopub.execute_input":"2025-04-19T03:06:41.388205Z","iopub.status.idle":"2025-04-19T03:06:42.711154Z","shell.execute_reply.started":"2025-04-19T03:06:41.388186Z","shell.execute_reply":"2025-04-19T03:06:42.710359Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded raw test dataset with 8000 examples\n✅ Converted to HuggingFace Dataset format\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b515b0dd270e44cd9e715ce04dd03fb2"}},"metadata":{}},{"name":"stdout","text":"✅ Tokenization complete\n✅ Created DataLoader with 125 batches\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print(\"Starting prediction generation...\")\nmodel.eval()\nall_predictions = []\n\ntotal_batches = len(test_dataloader)\nprint(f\"Total number of batches to process: {total_batches}\")\n\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(test_dataloader, 1):\n        batch = {k: v.to(device) for k, v in batch.items()}  # Move to GPU\n        outputs = model(**batch)  # Generate logits\n        predictions = torch.argmax(outputs.logits, dim=-1)  # Get predicted labels\n        all_predictions.extend(predictions.cpu().numpy())  # Store predictions\n        \n        # Print progress at every 10th batch\n        if batch_idx % 10 == 0 or batch_idx == total_batches:\n            print(f\"Processed batch {batch_idx}/{total_batches} ({(batch_idx/total_batches*100):.1f}%)\")\n            print(f\"Current predictions shape: {len(all_predictions)}\")\n            print(f\"Sample predictions from this batch: {predictions[:5].cpu().numpy()}\")\n            print(\"-\" * 50)\n\nprint(\"\\nPrediction generation completed!\")\nprint(f\"Total predictions generated: {len(all_predictions)}\")\nprint(f\"Unique classes predicted: {np.unique(all_predictions)}\")\nprint(f\"Class distribution: \\n{pd.Series(all_predictions).value_counts().sort_index()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T03:06:42.712963Z","iopub.execute_input":"2025-04-19T03:06:42.713196Z","iopub.status.idle":"2025-04-19T03:07:43.945985Z","shell.execute_reply.started":"2025-04-19T03:06:42.713180Z","shell.execute_reply":"2025-04-19T03:07:43.945399Z"}},"outputs":[{"name":"stdout","text":"Starting prediction generation...\nTotal number of batches to process: 125\nProcessed batch 10/125 (8.0%)\nCurrent predictions shape: 640\nSample predictions from this batch: [3 1 1 2 2]\n--------------------------------------------------\nProcessed batch 20/125 (16.0%)\nCurrent predictions shape: 1280\nSample predictions from this batch: [0 2 1 2 2]\n--------------------------------------------------\nProcessed batch 30/125 (24.0%)\nCurrent predictions shape: 1920\nSample predictions from this batch: [0 2 3 2 1]\n--------------------------------------------------\nProcessed batch 40/125 (32.0%)\nCurrent predictions shape: 2560\nSample predictions from this batch: [1 1 2 0 3]\n--------------------------------------------------\nProcessed batch 50/125 (40.0%)\nCurrent predictions shape: 3200\nSample predictions from this batch: [3 0 2 1 0]\n--------------------------------------------------\nProcessed batch 60/125 (48.0%)\nCurrent predictions shape: 3840\nSample predictions from this batch: [2 3 3 2 3]\n--------------------------------------------------\nProcessed batch 70/125 (56.0%)\nCurrent predictions shape: 4480\nSample predictions from this batch: [3 3 1 2 3]\n--------------------------------------------------\nProcessed batch 80/125 (64.0%)\nCurrent predictions shape: 5120\nSample predictions from this batch: [2 2 0 3 3]\n--------------------------------------------------\nProcessed batch 90/125 (72.0%)\nCurrent predictions shape: 5760\nSample predictions from this batch: [1 3 3 2 1]\n--------------------------------------------------\nProcessed batch 100/125 (80.0%)\nCurrent predictions shape: 6400\nSample predictions from this batch: [3 3 3 3 2]\n--------------------------------------------------\nProcessed batch 110/125 (88.0%)\nCurrent predictions shape: 7040\nSample predictions from this batch: [2 3 1 3 1]\n--------------------------------------------------\nProcessed batch 120/125 (96.0%)\nCurrent predictions shape: 7680\nSample predictions from this batch: [2 1 2 2 3]\n--------------------------------------------------\nProcessed batch 125/125 (100.0%)\nCurrent predictions shape: 8000\nSample predictions from this batch: [2 0 1 3 0]\n--------------------------------------------------\n\nPrediction generation completed!\nTotal predictions generated: 8000\nUnique classes predicted: [0 1 2 3]\nClass distribution: \n0    1552\n1    2000\n2    1827\n3    2621\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"df = pd.DataFrame({\n    \"ID\": range(len(all_predictions)),\n    \"label\": all_predictions\n})\ndf.to_csv(\"submission.csv\", index=False)\nprint(\"✅ Predictions saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T03:07:43.946615Z","iopub.execute_input":"2025-04-19T03:07:43.946797Z","iopub.status.idle":"2025-04-19T03:07:43.979591Z","shell.execute_reply.started":"2025-04-19T03:07:43.946783Z","shell.execute_reply":"2025-04-19T03:07:43.979067Z"}},"outputs":[{"name":"stdout","text":"✅ Predictions saved to submission.csv\n","output_type":"stream"}],"execution_count":30}]}